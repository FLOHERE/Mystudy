목차
- Hadoop 개요
- HDFS
- MapReduce 프레임 워크
- 가용성 향상
- 클러스터 모니터링
- Hadoop 운영
- 복수 사용자에 의한 리소스 제어
- 분산형 데이터 베이스 HBase
- 로그 수집기
- YARN

## Hadoop 개요
- 정의 : 대량의 데이터를 처리하기 위한 분산 처리 소프트웨어
	- 대량의 데이터 : 테라바이트~페타바이트급
	- 병렬 분산 처리 : 하나의 처리를 복수의 서버에서 동시처리
	- 가상화 기술 : 한대의 서버 -> 가상의 복수 서버로 인식(그림자 분신술)
	- Hadoop의 가상화 기술 : 복수의 서버 -> 하나의 가상 서버
- 분산 파일 시스템과 연계 -> 높은 스루풋
	- HDFS(분산 파일 시스템)
		- 대용량 데이터 -> 복수의 서버에 저장
		- 복수의 서버를 조합 -> 사용자에게 큰 파일 시스템 제공
	- MapReduce(병렬 분산 처리 프레임 워크)
		- Hadoop은 하나의 큰 처리(job)을 복수의 단위(task)로 분할해서 실행하는 구조이다.

>[!note] 쓰루풋
>데이터 처리 및 전송 효율성

- 자바 기반 -> 일반적인 서버에서 동작
	- IA(Intel Architecture) : 일반적인 서버와 네트워크 구축이 가능, 자바로 개발 -> 다양한 OS에서 동작
- Scale out : 서버 추가(= 용량 + 성능 향상)
	- Hadoop에서는 클러스터 구성하는 서버 대수를 추가 -> 저장 용량을 확장, 분산 처리 성능 향상
- Sclae Up : 서버 자체의 성능 향상
	- 성능 올리는데 한계
	- Scale out이 더 효율 좋다.

> 관계형 데이터 베이스나 검색 엔진과는 다르다.
> Hadoop = 대량의 데이터 -> batch(일괄처리)를 실행.
> 트랜잭션 제어 기능 X
> 검색 엔진 소프트웨어는 아니다.

### Hadoop 모듈과 프로젝트
- Hadoop 모듈
	- Hadoop Common : 다른 Hadoop 모듈을 지원하는 공통 유틸리티
	- 분산 파일 시스템(HDFS) : 애플리케이션에 대한 높은 처리량 액세스를 제공하는 분산 파일 시스템
	- YARN(Yet Another Resource Negotiator) : 작업 예약 + 클러스터 리소스 관리 프레임 워크
	- MapReduce : 대용량 데이터 세트의 병렬처리 프레임워크
	- Ozone : Hadoop용 분산 개체 저장소
- Hadoop 프로젝트
	- Ambari : Apache Hadoop 클러스털르 프로비저닝, 관리, 모니터링 하기 위한 웹 기반 도구
		- 히트맵과 같은 클러스터 상태를 볼 수 있는 대시보드
		- 사용자 친화적인 방식으로 성능 특성을 진단하는 기능
		- MapReduce, pig , Hive 애플리케이션을 시각적으로 볼수있는 기능 제공
	- AVRO : RPC(Remote Procedure Call)및 데이터 직렬화 프레임 워크
	- Cassandra : 단일 장애 지점 없는 확장 가능한 다중 마스터 데이터 베이스
	- Chukwa : 대규모 분산 시스템을 관리하기 위한 데이터 수집 시스템
	- Sqoop(SQL-to-Hadoop) : DBMS와 Hadoop 사이의 데이터 이동을 위한 Connector
		- 데이터 이관 지원
		- HDES, 하이브, Hbase에 임포트 하거나, 반대로 관계형 DB로 익스포트 할수 있다.
	- Hive : 데이터 요약 + 임시 쿼리를 제공하는 데이터 웨어하우스 인프라
	- Mahout : 기계 학습 + 데이터 마이닝 라이브러리, 협업 필터링, 클러스터링, 분류 기능
	- Pig : 병렬 계산을 위한 높은 수준(High Level)의 데이터 흐름 언어 및 실행 프레임 워크
	- Spark : Hadoop 데이터를 위한 빠르고 일반적인 컴퓨터 엔진.
		- ETL 기계학습, 스트림 처리, 그래프 계산을 포함한 관범위한 애플리케이션 지원하는 프로그래밍 모델 제공
	- Submarine : 분산 클러스터에서 머신 러닝 및 딮 러닝 워크로드를 실행하는 통합 AI 플랫폼
	- Tez : Hadoop YARN 기반, 일반화된 프로그래밍 프레임 워크
		- Hadoop 에코 시스템의 Hive, Pig와 상용 소프트웨어 기본 실행 엔진(Hadoop MapReduce)를 대체하는 솔루션
	- Zookeeper : 분산 애플리케이션 자원 조정 서비스 제공

- Hadoop 용도
- 대량의 데이터를 고속으로 처리

### Hadoop과 구글 기술과의 관계
- 분산 파일 시스템
	- 구글 : GFS:Google File System
	- Hadoop : Hadoop Distribute File System
- 분산 처리 프레임 워크
	- 구글 : MapReduce
	- Hadoop : Hadoop MapReduce
- 키 밸류형 데이터 스토어
	- 구글 : BIg Table
	- Hadoop : HBase

- 구글 파일 시스템
	- 대량의 데이터를 복수의 서버에 저장(=Hadoop)
- 맵 리듀스
	- 구글 파일 시스템에 저장되어 있는 대량의 데이터를 분산 처리 하기 위한 프레임 워크

### 병렬 분산 처리 시스템
- 기존 문제점
	- 병렬 분산 처리는 새로운 기술이 아니었으나, 특정 하드웨어에서만 동작 or 특정 소프트웨어에서만 동작
	- 매우 고가, 접근 힘듬

- Hadoop의 병렬 분산 처리 기술의 문제 해결 방안
	- Hadoop은 비싼 하드웨어가 아닌 일반적인 하드웨어에서도 동작함
	- Hadoop 설치가 용이한 IA 서버를 전제로 한 설계는 비용대비 효율이 높다.
	- 일부 서버가 고장이 나도 데이터를 잃거나 처리 실패가 일어나지 않도록 구성되어 있다.
	- MapReduce는 프레임 워크에 따라 애플레케이션을 개발하면 자동으로 분산 처리 애플리 케이션의 문제를 해결할수 있다.

#### 잡의 과제
1. 실행하고 싶은 처리(job)을 어떻게 테스크에 분배할것인가.
2. 어떤 테스크가 어떤 데이터를 처리할것인가?
3. 어떤 테스크를 어떤 서버에서 실행할 것인가?
4. 타스크 처리 중 서버가 고장난 경우 : 타스크 재실행

### 대규모 데이터 발생과 처리 방안
- 데이터 규모 증가
	- 유튜브 등등 겁나 많아짐(2020년 기준 35ZB)
- 대량의 데이터 처리 방안
	- 대용량 데이터를 효율적으로 읽어야 함 -> 복수의 디스크에 데이터 저장 -> 각 디스크에서 병행해서 읽음
		- HDFS는 각 서버의 내장 디스크에 데이터 저장
		- 대용량 파일 -> 작은 단위(블록)으로 분할
		- 복수의 서버 배치
		- 파일 크기가 1GB면 16개의 블록으로 분할하여 복수의 서버에 배치
		- 16개의 블록이 모두 개별 서버 배치 -> 효율 좋음
	- 파일을 읽어들일 떄는 각 서버에서 동시에 읽음 -> 높은 스루풋 실현

- 기존 시스템의 문제점
	- HDFS는 높은 스루풋으로 데이터 읽음.
	  but 저장장치와 서버를 네트워크로 연결하면 통신회선이 가진 성능 이상으로 스루풋 구현 X
	- 복수의 디스크 사용 -> 높은 처리량으로 데이터를 읽어도 해당 데이터를 네트워크를 통해 전송 -> 통신 속도 이상의 스루풋 구현 X
- 읽어들인 서버에서 데이터를 네트워크에 전송하지 않고 처리하는 것이 관건.
  Hadoop에서는 각 서버에서 읽은 데이터를 가능한한 그 서버내에서 처리한다.

- 데이터 지역성
	- 데이터 접근이 사간적, 공간적으로 가깝게 일어나는것.
	- 읽어들인 데이터를 네트워크를 통해 다른 서버에 전송 X -> 데이터가 있는 서버에서 태스크 처리
	- 16개의 블록 -> 16개의 서버에서 개별처리, 이후 하나로 합침.

- Hadoop의 미들웨어 기능
	- 병렬 처리 기능 : 복수의 디스크나 서버 동시 사용 O
	- 태스크 분할, 실패시 복원 기능 : 반드시 필요한 공통기능 제공

### Hadoop 적용 분야
- RDBMS / Hadoop비교
- RDBMS
	- ~수 테라 바이트
	- 작은 데이터 참조, 변경
	- 빠름. 온라인 처리에 적합
	- 여러대의 서버로 스케일 업
	- 서버 자체의 성능 향상에 초점
	- 구조화 데이터
	- 사전에 정의된 구조로 정규화
- Hadoop
	- 테라바이트~페타바이트
	- 큰 데이터 삽입, 참조(변경 없음)
	- 느림, 일괄 처리에 적합
	- 수백대~수십대의 서버로 스케일 아웃
	- 확장성이 쉽다
	- 준 구조화 데이터
	- 사전에 데이터 구조를 정의할 필요가 없다.
- 실제 시스템에서는 둘다 합쳐서 쓴다.

## Hadoop 시스템 구성과 아키텍쳐
### 서버 구성
- 마스터 서버 : 클러스터 전체를 관리
	- 슬레이브 서버보다 고성능, 고신뢰 장비 사용
	- SAS(Serial Attached SCSI)디스크 이용한 RAID 구성이 적합
- 슬레이브 서버 : 실제로 데이터 저장,관리
	- RAID필요 X
	- 이들 서버군이 협력하여 동작 = 하둡 클러스터 구성

### HDFS 마스터 서버(=NameNode)
- 클러스터 전체에 걸쳐 데이터가 어디에 있는지 메타데이터 관리
- 슬레이브서버 = DataNode : 데이터를 읽고 쓰는 역할을 함

### MapReduce 마스터 서버(=JobTracker)
- 잡을 복수의 태스크 처리로 분할 -> 각 슬레이브 서버에 할당
- 슬레이브 서버 = TaskTracker : 할당된 태스크 실행, 결과 반환

#### NameNode / JobTracker
- 각각 한대씩
- 소규모 Hadoop 클러스터에서는 NameNode와 JobTracker가 하나의 장비내에서 동작
- 대규모 클러스터에서는 별도 장비 사용

#### DataNode / TaskTracker
- 같은 서버에 설치되는 것이 일반적
- 이는 DataNode가 읽어들인 데이터를 TaskTracker로 처리하기 위한 것임.
![[70C2D899-4E0D-4140-BE8E-2A515819AD99_1_201_a.jpeg]]

### 랙과 Hadoop 클러스터 
- 랙 : 데이터 센터 내 서버
	- 네트워크 장치들을 적재하기 위한 표준 프레임 워크
	- 업링크 스위치 : 랙과 랙을 연결해주는 스위치(보통 40GB 대역폭 사용)

## HDFS : Hadoop 분산 파일 시스템
- HDFS : 슬레이브 서버에 있는 로컬 파일 시스템 상에서 만들어지는 파일 시스템
	- HDFS에 배치된 파일은 64MB 단위로 분할 -> 각 노드에 저장. 로컬 파일시스템 취급
- HDFS 장점
	- 투과성 : 파일 시스템 내부 복수의 서버 동작 -> 로컬파일 다루듯 투과적 접근
		- 파일이 어떻게 블록으로 분할되었는지 의식할 필요X
	- 확장성 : HDFS는 DataNode 대수를 늘려서 용량과 성능 향상
		- 디스크 용량이 부족하다 => 서버 수 늘리면 된다.
	- 신뢰성 : 하나의 파일 -> 복수의블록으로 분할 -> 복수의 서버에 다중 기록(Replication)
		- 기본 설정 : 각 블록이 3개의 서버에 다중저장됨.
		- 하나의 서버 고장 -> 블록에 접근 X 
		  but! 다른 서버에 동일한 블록이 존재 -> 분산파일 시스템 전체적으로 데이터 소실 위험 낮음
		- NameNode가 고장나면 HDFS 전체 고장 -> 가용성을 높이기 위해 HA 구성
		- 일부 서버 고장으로 블록에 접근이 안되면 해당 블록을 별도 서버에 복제, 설정된 다중도를 만족시킴(안전성 확보).
	- 접근 패턴 제한
		- 연속적 스트림 읽기 전제
		- 랜덤 읽기 방식 고려X
		- HDFS데이터 기록은 한번만 이루어짐.
		- 기록 이후 읽기 처리만 가능
		- 데이터 변경 불가능

- NameNode 의 역할
	- DataNode 상태 감시
	- 블록 관리
	- 메타데이터 관리
- DataNode
	- 블록 저장
	- NameNode와 통신 : DataNode의 동작 여부와 관리하고 있는 블록 정보를 NameNod에 전달
- Client
	- 파일 접근시 각 블록이 어떤 DataNode에 배치되어 있는지 NameNode에게 물어봄

>[!note] 메타데이터
>파일 이름, 디렉토리 정보, 복제본 개수, 파일을 구성하는 데이터 블록들의 id리스트, 
>각 블록별 저장 데이터노드의 이름
>(쉽게 말해서, 식료품 포장지의 유통기한, 원산지, 알러지 유뮤 등등이 적혀있는 것과 비슷한 맥락)

- MapReduce 프레임워크
	- 대규모 데이터 집합을 처리하기 위한 프로그래밍 모델
	- 잡 처리를 병렬로 실행 -> 하나의 잡을 독립된 테스크로 나누어 실행
- MapReduce 처리 흐름
	- Map(맵)처리 / Reuduce(리듀스)처리로 나눠짐
	- 각각 어떤 동작을 할것인지 정의
	- 이후 MapReduce프레임 워크가 자동으로 병렬분산 처리 실행.
- Map처리 : 입력 파일을 한줄씩 읽어서 필터링
	1. 입력데이터를 분할 Map 태스크에 전달
	2. Map태스크가 병행하여 각 데이터를 처리, 작업 분담을 위한 키 설정
-  Shuffle : Map 과 Reduce 사이에 있는 처리로, 자동으로 실행된다.
	3. Map 태스크별로 같은 키를 가진 데이터 병합 -> 다시 한번 키별로 데이터 모음
 - Reduce 처리 : 데이터 집약
	 4. Reduce 태스크가 병행하여 키 별 데이터 처리 수행

- 어떤 처리든 데이터는 Key 와 Value 로 이루어짐.
	- 입력데이터와 출력 데이터도 마찬가지.

>[!note] 프레임워크
>소프트웨어 개발을 위한 구칙, 도구를 제공하는 뼈대

- 포멧/이미지 변환
	- 어떤 포맷의 데이터 -> 다른 포맷으로 변환 하는 작업이 대량일 경우, Hadoop 사용
	- [[기계학습]]

### Hadoop 도입
- MapReduce버전
- 1.0계
	- JobTracker 와 TaskTracker를 채용
		- JobTracker : 잡 실행 관리
		- TaskTracker : 개별 태스크 실행하는 잡 실행 프레임워크
- 2.0계
	- 아키텍쳐 계층에 새로운 실행 프레임 워크YARN을 채용
	- 리소스 관리 및 잡 스캐쥴/감시 <- 두가지로 분할

### Hadoop 동작 모드
- 로컬 모드
	- 한대의 서버상에 HDFS를 사용하지 않고 MapReduce 동작 환경 구축
- 유사 분산 모드
	- 한대의 서버상에 HDFS를 사용한 MapReduce 동작 환경 구축
	- 네트워크 통신 발생 X
- 완전 분산 모드
	- 여러대의 서버상에 HDFS를 사용한 MapReduce동작 환경 구축
![[빅데이터 개론 기말고사 범위.pdf]]

## Hadoop 파일 시스템
- 개요
	- Hadoop 구성 요소 중 가장 큰 부분을 차지하는것 = 파일시스템(데이터 저장 장소)
	- 여러대의 노드상에 하나의 파일 시스템 공간 생성
		- 각 노드에 OS가 제공하는 범용 파일 시스템 존재.
		- HDFS는 범용 파일 시스템 위에서 동작
		- 지정된 디렉터리 이하를 데이터 영역으로 이용, 각 노드상 디스크 공간을 합쳐 거대한 파일 시스템 생성
		- HDFS를 사용하는 클라이언트는 어떤 데이터 노드가 있는지 신경쓸 필요 X.
		- 파일 시스템상의 데이터 읽고 쓰기는 가능
		- HDFS는 대용량 데이터를 다룩 위해 특화된 아키텍쳐.
		- 하나의 파일 = 수기가 바이트 이상 -> 고속처리 가능

### HDFS 특징
- 대용량 데이터 범용 서버만으로 처리 가능
- 파일 크기수 : 수기가~수테라 바이트
- 대용량 데이터 저장은 IA 서버만으로 실현 가능
- 파일 시스템 크기에 제한 X

- 용량 확장성
	- 노드 추가만으로 영역 크기 쉽게 늘리기 가능
	- 시스템을 단계적으로 확장 -> 불필요한 투자 방지

- 순차적 접근, 높은 처리량 실현
	- 일괄처리 적합 
	- 처리속도 < 처리량 중시
	- "쓰기는 한번만, 읽기는 여러번 가능"
	- 대량의 데이터에 빠른 속도로 접근하는데 특화 -> 순차적 접근 패턴만 제공
	- 대량의 데이터 모아서 한번에 처리
	- 갱신이 필요 없는 대량의 로그 축적에 적합
- 슬레이브 노드 일부가 고장나도 데이터 손실 방지
	- 여러 대의 노드로 구성
	- 복수의 노드 사용 -> 데이터 복제 유지 => 손실 방지

## HDFS의 구조
- 여러대의 서버상 하나의 파일 시스템 공간 생성
- 마스터-슬레이브 구성
	- 마스터 서버 : NameNode
		- HDFS 전체 관리. NameNode에서 다루는 파일의 메타데이터 관리는 SecondaryNameNode가 수행
	- 슬레이브 서버 : DataNode
		- 사용자가 DataNode 상에 투입한 파일의 데이터 블록 저장.
		- 파일을 일정 크기로 분할 -> 복수의 DataNode상에 분산해서 배치됨.
- 사용자는 HDFS 클라이언트를 경유하여 HDFS에 데이터 기록.

- 각 DataNode에 데이터 배치와 HDFS의 관계
	- 각 DataNode상 리눅스 등 OS가 있고, 그위에 ext4등의 파일 시스템 존재.
		- 해당 파일 시스템내의 어떤 디렉토리를 HDFS용으로 사용할지 미리 지정.
	- HDFS = DataNode상 HDFS용 영역 합침 => 파일 시스템 형성
	- 파일 시스템 : OS가 제공하는 기능중 하나, HDFS는 자바로 구현된 미들웨어이기 때문에 os보다 상위 계층에서 제공
- HDFS상 파일과 블록
	- 데이터가 블록 단위로 분할,관리
	- 복수 블록으로 구성
	- DataNode 로컬 파일 시스템 파일에 대응
	- 블록 크기에 해당하는 영역 확보

### DataNode의 역할
1. 데이터 영역 할당
2. 블록 구성
3. 블록 리플리케이션
4. Rack awareness기능 수행

- DataNode에 데이터 영역 할당
	- 각 DataNode상 HDFS용 디렉터리 지정 사용
	- HDFS : 각각의 DataNode상의 HDFS영역을 합쳐서 파일 시스템 형성
- 블록 구성
	- HDFS상에 배치되는 데이터 -> 블록 단위로 분할 관리
	- HDFS상에 저장하는 파일 -> 복수의 블록으로 구성
	- 일반적인 파일 시스템 : 블록 내 저장되는 데이터 크기 < 블록 -> 디스크 상에 블록크기에 해당하는 영역 확보
	- HDFS : 1MB 파일 기록 -> 디스크 영역도 1MB만 확보(파일 크기에 맞춰서 영역 확보)
- 블록 리플리케이션(Replication)
	- 각 블록을 여러대의 DataNode상에 복제본을 배치하는것.
	- 리플리케이션 수 : 3으로 설정
		- 같은 블록 3대의 DataNode상에 존재. 
		- 특정 DataNode에 블록 배치 
		--> 다른 DataNode에 복제본 전송
- Rack awarness
	- 복제본 3개가 동일 렉에 있는 DataNode에 3대에 배치 
	--> 해당 렉에 연결된 스위치 장애에 의해 모든 복제본 잃음. 
	- 이를 방지하기 위해 복제본이 동일상에 존재하지 않도록 조정하느것을 말함.

### NameNode의 역할
- NameNode
	1. HDFS가 관리하는 메타데이터 관리
	2. HDFS 사용 현황 확인
	3. 블록의 복제본수 관리
	4. 클라이언트의 HDFS 처리 요청 접수
	5. DataNode의 다운 여부와 감시 기능 수행

#### 메타데이터 관리
- 메타데이터:  파일 시스템 이미지(fsimg), DataNode의 블록 대응 정보
	- 블록들의 id 리스트, 각 블록을 저장하고 있는 DataNode의 이름
	- 파일 시스템 이미지 : 파일 속성 정보, 디렉터리 구조 등
	- 파일의 종류를 알려줌
	- 데이터는 DataNode가 분산해서 가지고 있음 , but NameNode가 메타데이터를 기반으로 HDFS를 관리
	- 메모리에서 관리됨
	- 디스크 접근 X, 메모리상 데이터만 사용해서 관리 -> 응답시간 매우 빠름

#### HDFS 사용 현황 및 관리
- HDFS 영역이 다참 -> 관리자가 블록 용량에 여유가 있는 다른 DataNode로 블록 이동

#### 블록의 복제본 수 관리
- 복제본 수가 사전에 설정해둔 값과 일치X -> 복제본 새로 만들거나 삭제
- DataNode에 장애 발생 -> 블록 손실 -> 새로운 복제본 생성

#### 클라이언트로 부터 요청 접수
- 클라이언트 -> HDFS 파일 접근 요청 -> NameNode에 맨 먼저 접속 -> 도착 -> 대상 블록을 가지고 있는 DataNode 리스트 클라이언트에 전달 -> 리스트 기준 DataNode와 소통

#### DataNode 다운 여부 감시
- DataNode -> NameNode 로 하트비트 패킷을 일정간격으로 전송
	- 하트비트 내용 : DataNode가 가지고 있는 보유 데이터 블록 리스트. NameNode는 하트비트를송신하지 못하는 DataNode에 고장 판단 내림
>[!note] 하트비트(HeartBeat)
>자신의 생존 여부를 전달하는 하나의 데이터 전송 과정

## HDFS의 파일 I/O 흐름
- 개요
	- 사용자가 데이터 투입 할때, HDFS 클라이언트 사용.
	- HDFS클라이언트 -> NameNode와 처음 통신
	  이때 , 어떤 DataNode와 작업하면 되는지 파악
		- 데이터 교환 : NameNode를 거치지 않고, 클라이언트와 DataNode가 직접 데이터 교환, 실제 디스크 I/O는 DataNode가 수행
	- NameNode가 DataNode에 대해 I/O 요청, 발행 X
		- NameNode에 부하 X
	- HDFS 데이터 저장 : 맨 처음과 맨 마지막 처리, 에러 발생시에만 NameNode와 통신

### HDFS 파일 저장하기
- 전송큐와 ack큐 사용 (9단계)
	- 전송큐 : 저장 데이터 관리
	- ack큐 : 저장 상태 관리
1. 파일 저장을 위한 파일 열기 요청
2. 허가용량 체크와 메타데이터에 파일 엔트리
3. 파일 저장용 스트림 전달
4. 전송 큐 패킷 저장
5. 블록 할당을 요구
6. 블록 저장대상 DataNode 리스트 전달
7. ack큐에도 동일 패킷 저장
8. 파이프라인에 포함된 DataNode에 패킷 전달
9. 저장 끝나면 ack큐에서 패킷을 제거

#### 클라이언트와 NameNode 
1. 클라이언트 -> NameNode에 저장을 위한 파일 열기 요청 하고, NameNode에 파일 작성 요구
2. 요구 접수한 NameNode 허가 용량 체크
	- 파일 저장 가능한 상태인지 확인 후
	- 메타데이터 파일 엔트리 추가
	- 이후 클라이언트는 데이터 전송 큐 감시
>[!note] 파일 엔트리
>파일 이름, 크기, 블록의 위치 등이 포함
>HDFS 클러스터에서 파일 및 블록의 위치를 추적

#### 저장 개시
3. 클라이언트가 저장용 스트림 받으면 클라이언트가 파일 저장 시작
4. 받은 데이터 -> 패킷 단위로 전송큐에 삽입
5. 데이터 전송 큐 안에 패킷 삽입 확인 -> NameNode에 블록 할당 요구
6. 패킷 저장 대상 DataNode 리스트 수신
	- 이 리스트에서 복제수와 동일 수의 DataNode를 연결한 파이프 라인 형성
	- 데이터 전송큐 -> 패킷을 꺼내 DataNode에 기록
7. 별도 스레드로 관리하고 있는 ack 대기 큐에 패킷을 저장

#### 패킷 전달
8. 패킷이 기록된 DataNode는 파이프라인에 포함된 다른DataNode에 패킷을 차례로 전달
	- 패킷 전달 => Rack awareness를 포함하는 리플리케이션 구칙을 따른다.
	- 두번째 복사본을 기록할때는 첫번째 복제가 저장된 DataNode와 다른곳에 있는 DataNode에 패킷 전달.
9. 세번째 복제본은 두분째 복제본이 전달된 DataNode와 동일에 있는 DataNode에 전달된다.
	- 패킷이 정상적으로 기록된 DataNode는 ack메세지를 파이프라인 상부에 통지
	- ack메세지는 파이프라인을 전달, 최종적으로 클라이언트에 도착
	- ack가 도착 -> 대응패킷이 ack큐에서 제거
- 파이프라인에 따른 데이터 저장은 노드가간 비동기로 진행
	- 파이프라인을 닫을때 해당 블록 저장이 완료되고 ack가 돌아올때까지 기다림

#### 저장중 장애 발생
- ack큐 내부에 있는 패킷을 모두 데이터 전송큐로 이동
- 장애가 발생한 DataNode를 제외시킨 파이프라인을 형성 -> 다시 파이프라인에 저장
- 블록 크기만큼 패킷을 저장한 단계 -> 새로운 블록 할당을 위해 클라이언트 다시 NameNdoe로 부터 DataNode리스트 취득 -> 새로운 파이프라인 형성
- 클라이언트 반복 -> 모든 데이터 저장 -> 작업 끝

## HDFS
### HDFS에서 파일 읽기
1. 파일 열기와 DataNode 리스트요구
2. 읽을 대상 블록과 DataNode리스트 반환
3. DataNode에서 블록 읽기
4. DataNode에서 블록 읽기를 반복
- 클라이언트와 NameNode와의 통신, 읽기 개시 순으로 진행, 블록을 읽을때는 클라이언트와 가장 가까운 DataNode를 선택

#### 클라이언트와 NameNode의 통신
1. HDFS에서 파일 읽을 때, NameNode에 읽기 전용 파일 열기 요청
	- 요구한 파일 블록을 가진 DataNode 리스트 요청
2. 클라이언트로 부터 요청 받은 NameNode는 오픈때 부여한 파일 경로에서 해당 파일을 구성하는 블록 리스트를 생성
	- 해당 블록의 DataNode리스트 -> 클라이언트에 전달

#### 읽기 개시
3. 클라이언트는 취득한 정보로부터 파일읽기 개시
	- 최초 블록의 DataNode에 접속 -> 블록 읽음
	- 블록 읽을때 클라이언트와 가장 가까운 쪽의 DataNode 선택
4. 블록의 마지막 부분까지 읽음 -> 다음 블록을 가진 DataNode에 접속 -> 다시 블록 읽음
	- 반복
	- 파일 읽기 끝 -> 클라이언트 : 파일 닫기 요구

### 파일 시스템 메타데이터
-  메타데이터에 포함되는 정보
	1. 파일 시스템 이미지(파일 속성 정보, 디렉터리 구조 등)
		- 파일 속성 정보 
			- 파일명 
			- 부모 디렉터리
			- 파일크기
			- 소유자와 소속 그룹
			- 파일 읽기/쓰기 속성
	2. DataNode의 블록 대응 정보
		- 블록 ID
		- 블록을 보유하고 있는 DataNode에 대한 정보
			- 하트비트
				- DataNode는 초기 설정에서 3초 간격으로 NameNode에 하트비트 전송
				- 자신이 관리하고 있는 블록 NameNode에 통지
			- 블록 리포트
				- NameNode는 DataNode의 블록리포트를 바탕, 메타데이터 내의 블록 대응 정보를 구축, 복제수가 충분한지 판단
				- DataNode가 자신이 관리하고 있는 블록들을 말함
- fsimage(file system image): 체크 포인트 타이밍에 로컬 파일 시스템에 기록
	- 메모리상 관리되고 있는 메타데이터 내 파일 시스템 이미지는 체크포인트 타이밍에 NameNode의 로컬 파일 시스템에 생성
	- fsi -> DataNode와 블록 대응정보 포함 X
		- 블록이 항상 같은 DataNode에 의해 관리 X
	- HDFS는 파일 변경을 트랜잭션
		- 파일 처리 시마다 NameNode의 메모리와 로컬 파일 시스템에 editlog(파일 편집 로그) 생성

>[!note] 체크포인트
>60번에 한번씩 정보를 저장 하는것.

- edits : 파일 편집 로그
	- edits(editlog) : 로컬 파일 시스템에 생성, HDFS에 기록한 편집 이력.
		- edits를 메모리 상에서 관리되고 있는 파일 시스템 이미지에 적용 -> 이미지 최신 상태 유지
	- 체크포인트와 체크포인트 사이의 모든 HDFS 트랜잭션은 editlog라는 파일에 수록
		- 체크포인트 시마다 editlog는 리셋됨.
	- editlog 용도 : 체크포인트와 체크포인트 사이의 문제 발생에 대비해 기록
- 메모리상의 fsi와 데스크에 있는 fsi의 동기화 시점
	- 체크포인트(일정 타이밍)에만 메모리 내의 fsi디스크가 동기화 됨.
	- 메모리에 있는 메타데이터에 editlog가 update되어 최신화 된 fsi를 로컬디스크이 fsi로 복제
	- fsi와 edits : NameNode가 관리하고 있는 데이터
- 디스크 동기
- edits는 HDFS에 무언가 기록할때마다 메모리와 로컬 파일 시스템 내용 동기화
	- 최신 변경 상태를 기록하고 있는 edits 사용 -> fsimage 최신화 + HDFS파일 시스템 이미지 구축 가능
	- 편집 이력인 edits는 1회 조작에 대한 로그가 크지 않고, 순차적으로 기록됨. -> 편집할때마다 로컬 시스템에 생성한다고 해도 성능에 영향 X
	- edits르 ㄹ로컬 파일 시스템에 기록 -> 신뢰성 확보
- editlog 사용하는 이유(HDFS에 무언가 기록할때마다 로컬 파일 시스템에 fsimage를 생성하지 않는 이유)
	- fsimage가 수기가 바이트 넘을 가능성이 있으므로, HDFS에 기록할때 마다 fsimage을 로컬 파일 시스템에 생성하면 대량의 디스크 I/O 발생 -> 성능에 악영향

- 데이터 처리 시 흐름
	- NameNode 시작시 흐름
		1. NameNode시작시에 로컬 디스크 fsi로 부터 HDFS 메타데이터 구축
		2. NameNode 시작시에 메모리로 읽어들인 fsimage에 대해 edits 내용으 ㄹ적용해서 메타데이터 내용을 최신화
		3. 메타데이터 최신화 후, 디스크에 fsimage를 기록(체크포인트)
	- HDFS에 기록할때의 흐름
		1.HDFS 조작 시마다 편집 내용이 로컬 디스크의 edits에 생성
		2. 편집 내용을 메타데이터에 적용, 메모리상의 메타데이터를 항상 최신의 상태로 유지

- 클러스터 시작시의 메타데이터 구축
	- HDFS클러스터 시작 -> 메타데이터 구축 = 파일 시스템 이미지와 DataNode의 블록 대응 정보 구축
		- 파일 시스템 이미지 구축
			- NameNode 시작 직후 메모리에 fsimage를 로드 -> 여기에 edits를 적용 -> fsimage 최신화
			- edits 클수록 시간 걸림 -> Secondary NameNode 준비
		- 블록 대응 정보 구축
			- (클러스터 시작 후) NameNodedprp DataNode가 가지고 있는 블록 신고 -> 구축
- 최소 복제 수
	- HDFS 시작 후 전체 블록 99% -> 최소 복제 수에 도달 해야함.
- 안전모드
	- 최소 복제수에 도달하기 까지 안전모드(safe Mode)로 동작.
	- 안전모드에서는 HDFS가 읽기 전용이 되고 복제 X
	- 안전모드로 동작 해야 되는데 작동이 안되고 온라인이 되면 DataNode가 동작하는 상태로 판단,
		- NameNode는 복제수가 부족하다고 오판하여 동작하고 있는 다른 DataNode에 복제본을 만든다.(시스템 전체가 오작동)

- Secondary NameNode의 역할
	- NameNode로 부터 주기적으로 fsimage + edits 합침. 
		- HDFS에 대한 갱신 내용을 반영한 새로운 fsimage를 생성.
		- 합친 후 fsimage는 NameNode에 전송
- Non HA 구성 사용, NameNode와 별도의 서버상에 구성하는것이 좋음
- HA클러스터를 구성하지 않는 Hadoop 클러스터는 하나의 NameNode와 Secondary NameNode로 구성
	- 별도의 서버상에 동작 시키는것 추천
	  -> 이유 : 메모리나 다른 리소스를 병합해서 사용하지 못하도록 하고 , NameNode에 장애가 발생한 경우, NameNode 이외의 서버에 메타데이터 보존 -> 장애에 따른 완전 데이터 손실을 방지하기 위함.

## MapReduce 프레임 워크 
### MapReduce 프레임 워크 특징
- 데이터가 있는 서버로 코드전송
	- 처리할 데이터가 있는 서버로 코드 전송 -> 빠른 실행 속도
- 데이터를 키/벨류 데이터 셋의 변환 처리
	- 맵 태스크 : 입력 데이터 하나 이상의 조각(split)으로 나눈 후, 데이터 조각의 수만큼 서버에서 각각 병렬 처리
	- 리듀스 태스크 : 여러개의 맵 태스크부터 출력을 모아서 최종 처리하는 과정
- Share Nothing 아키텍쳐
	- 병렬성이 아주 높다. (맵 태스크와 리듀스 태스크는 의존성이 없으므로 각기 처리할 레코드만 보면됨)
- 오프라인 배치에 적합
	- 맵리듀스 => 시간 걸림, 기본적으로 대량의 데이터의 배치 처리를 위한 시스템

### MapReduce 처리
- 프레임 워크 처리 흐름
	- Map, Shuffle&Sort, Reduce 단계를 거쳐서 입력된 잡을 분할해가며 처리 진행
	- MapReduce 잡 : 복수의 태스크로 분할. Map 태스크와 Reduce 태스크는 각각 병렬로 처리
	- Map과 Reduce의 처리 : Map 처리가 끝난 후 Reduce처리 시작
	- Shuffle : Map 의 출력 결과를 Reduce에 전달하는 단계. Map 태스크가 끝난 후 순차적 실행
	- 최종 결과 : HDFS에 출력
	- Map 처리 입력 데이터 
		- 스플릿 (split) : jobClient가 스플릿 단위 정함.
		- 입력데이터는 HDFS 파일. 스플릿 크기 = 블록 크기
		- 스플릿에서 key value를 해석, 1레코드씩 읽어서 Map으로 처리
- Shuffle & Sort
	- Map 출력 -> Reduce ,중간 처리 과정
	- 키 벨류 -> 키 벨류 
	- 키 벨류로 받은거 -> 파티셔너(Partitioner) 통해서 Reduce처리하는 Reducer에게 전달
		- 파티셔너(Partitioner)
			- 파티션 : Reduce처리의 입력
			- Map 처리의 출력 결과(키-밸류)가 파티션으로 분할, 같은 키를 가진것 끼리 모음
			- Partitioner에 따라 목적지 Reduce가 정해짐
- Reduce 처리
	- Reduce 처리하는 노드 -> Map 처리 수행한 복수의 노드중 어딘가 포함
		- Reduce 처리는 Map 처리으 출력결과를 모두 전달받음
		- 대규모 클러스터에서 대용량 데이터 처리하는 경우 -> Map 처리 노드와 Reduce 처리 노드가 같을 확률 감소
		- 이때, Map 처리 결과, 많은 데이터가 네트워크 경유로 전송, 발생하는 대량의 네트워크 통신이 MapReduce 전체의 처리 성능 저하를 초래 할수도 있음
	- Reducer에 Map 으로부터 데이터가 도달하면 Reduce태스크에서 처리가 가능하도록 입력을 하나로 모으는 처리함.
	- 이때 복수의 Map에서 정렬된 조각들 전달 -> 키별로 병합 - 정렬 - 집약
		- Reduce태스크의 입력은 키별로 정렬된 상태 됨
- Reduce 처리에서 하는 작업
	1. 같은 키로 모아진 중간 데이터(벨류 집합)에 대해 사용자가 정의한 Reduce처리 실행
	2. Reduce처리 결과, 새로운 키, 밸류 쌍 생성 -> 출력

### MapReduce 아키텍쳐
- 노드 구성
	- MapReduce 처리 단위 : 잡(job)과 태스크
		- 잡 : 사용자가 관리 대상으로 하는 MapReduce처리 단위
		- 태스크 : Map처리나 Reduce처리에 할당되는 처리 단위.
			- 병렬처리가 가능하도록 MapReduce 프레임 워크가 분할
	- MapReduce 잡은 프레임워크 내에서 복수의 태스크로 분할하여 처리됨

### JobTracker의 역할
- 잡 관리
- 리소스 관리
- 잡 실행 이력 관리 수행

1. 잡 관리
	- Map 태스크 할당 제어 : jobClient가 보낸 분할 정보를 파악 -> Map 태스크 할당 제어
	- Map 처리 결과 파악 : Reduce 처리에서 이용할수 있는 Map 처리 결과 정보 파악
	- 잡 진행 통지 : MapReudce 잡의 진행상황 -> JobClient에게 통지
2. 리소스 관리
	- 처리 할당 : Map처리나 Reduce처리를 TaskTracker에게 할당
	- 처리의 주기적 실행 : 같은 태스크를 복수의 TaskTracker에게 병렬 실행 -> 가장 빨리 얻은 결과를 사용하도록 처리 할당
	- 처리 재할당 : TaskTracker에서 이상 발생 -> 처리 X -> 다른 TaskTracker에게 처리 재할당
	- 블랙 리스트화 : 처리 실패 빈도가 높은 TaskTracker에게 처리 할당X
	- TaskTracker 동작 여부 확인 : 동작여부 확인 후, 응답 없으면 작업처리에서 제외
	- TaskTracker의 추가 / 제외 : 신규로 추가한 TaskTracker에서 응답이 있는 경우 , 바로 처리용 맴버로 추가, 운영등의 목적으로 TaskTracker를 제외
3. 잡 실행 이력 관리 
	- TaskTracker의 역할 : JobTracker에서 받은 지시에 따라 Map처리나 Reduce처리 샐행
	1. 실제 작업하는 Child 프로세스 생성
		- 처리 내용을 저장한 JAR파일이나 처리에 필요한 데이터를 Child프로세스에게 전달
		- Child 프로세스의 상태 확인 : Child 프로세스의 진행 상태를 JobTracker에게 정기적으로 통지
	2. 하트비트 통신(HDFS에서)
		- 자신이 동작 하고 있다는 것을 JobTracker에게 정기적으로 하트비트 전송
		- 처리 상황이나 처리 유무에 대해 소통
		- 응답 없으면 작업처리에서 제외
	3. Map처리 수와 Reduce처리수 파악
		- 어느 한 시점 Map처리수나 Reduce처리수 파악
	4. Child프로세스가 처리 완료되면 TaskTracker가 JobTracker에게 처리 완료 알림
	5. 처리 중지
		- JobTracker로 부터 처리 중지 지시가 오면 해당 처리 중지

>[!note] JAR
>자바, 압축 프로그램으로 열어볼수 있는 압축파일 

- JobClient의 역할
	- 입력 데이터 분할 방침 결정 : 입력데이터를 어떻게 분할해서 병렬처리 할것인지 결정
	- Job 의뢰 : JobTracker에게 MapReduce 잡 실행 의뢰
	- 애플리 케이션 배포 : MapReduce잡을 실행하기 위한 애플리케이션 -> HDFS에 저장
	- 진행 상태 수신 : JobTracker가 보내온 잡 실행 상태 수신
	- 잡 관리
		- 사용자 단위로 MapReduce잡 관리 -> MapReduce잡의 우선순위 변경이나 잡을 강제 종료

## MapReduce와 HDFS의 관계
### 데이터 지역성
- 데이터 지역성
	- 처리 프로그램을 데이터가 있는 곳으로 이동시키는 것.
	- 데이터 전송량을 줄이고, 오버헤드 발생 X
- HDFS는 읽어들인 데이터를 네트워크를 통해 다른 서버에 전송하지 않고 데이터가 있는서버에서 태스크 처리함.

## 고가용성의 기본
### Hadoop의 가용성 보장을 위한 구조
- 데이터 블록 이중화
	- 블록을 이중화 해서 보존
	- MapReduce는 태스크를 별도 노드에서 재실행 -> 가용성 보장
- 마스터 노드 이중화
	- HA클러스터 구성
	- 클러스터 한대에 존재 -> 단일 장애 지점됨
	- 마스터 노드를 어떻게 이중화 하는지 중요
- 마스터 노드 가용성 향상에 필요한 기능
	- 마스터 노드가 가진 정보를 이중화 해서 저장하는 기능
	- 가동중인 마스터 노드 고장 -> 별도의 노드인식 -> 마스터 노드 처리 인계 가능
	- 문제 있음 -> 마스터 노드 변경

- 액티브 - 스탠바이와 페일오버(fail over)
	- 페일 오버 : 문제 발생 시 대기용 서버로 자동 전환되는 기능\
		- 페일 오버 처리시 액티브 노드(주 노드) 상태에서 가동중인 프로세스 정지, 파일 시스템 언마운트, 가상 IP 주소 제거등 해야함.
	- 펜싱(fencing) : 액티브 노드를 확실히 격리하는 처리(사전 처리)
		- 리소스 처리가 정상적으로 완료되지 않은 상태에서 스탠바이측 노드를 액티브 상태로 변경하면 복수의 노드에서 부정합 처리가 이루어져 잘못된 결과가 반환되거나 데이터 망가짐.